{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import torchvision\n",
    "from voc import *\n",
    "from coco import *\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet152, resnet101, resnet18, resnet34, resnet50\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from config import seed_everything\n",
    "seed_everything(0)\n",
    "import timm\n",
    "\n",
    "# from models import *\n",
    "from backbones.config import config\n",
    "import pathlib\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(config[\"swin\"], num_classes=80, pretrained=True).to(device)\n",
    "pre = torch.nn.Sequential(*[model.patch_embed,\n",
    "model.pos_drop])\n",
    "\n",
    "print(len(model.layers))\n",
    "post = torch.nn.Sequential(*[model.norm,\n",
    "# model.fc_norm,\n",
    "model.head])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9216, 128])\n",
      "torch.Size([1, 2304, 256])\n",
      "torch.Size([1, 576, 512])\n",
      "torch.Size([1, 144, 1024])\n",
      "torch.Size([1, 144, 1024])\n",
      "torch.Size([1, 144, 1024])\n",
      "torch.Size([1, 144, 80])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.randn(1, 3, 384, 384).to(device)\n",
    "\n",
    "ln = nn.LayerNorm(1024).to(device)\n",
    "def forward_(model, inp):\n",
    "  inp = pre(inp)\n",
    "  print(inp.shape)\n",
    "\n",
    "  int_li = []\n",
    "  for b in model.layers:\n",
    "    inp = b(inp)\n",
    "    print(inp.shape)\n",
    "    int_li.append(inp)\n",
    "\n",
    "  inp = ln(inp)\n",
    "  print(inp.shape)\n",
    "  inp = post(inp)\n",
    "  print(inp.shape)\n",
    "  return inp, int_li\n",
    "\n",
    "inp, int_li = forward_(model, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll1 = nn.Linear(256, 1, bias=False).to(device)\n",
    "ll2 = nn.Linear(512, 1, bias=False).to(device)\n",
    "ll3 = nn.Linear(1024, 1, bias=False).to(device)\n",
    "ll4 = nn.Linear(1024, 1, bias=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 2816])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 2816])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_kv(int_li):\n",
    "  res = None\n",
    "  softmax = nn.Softmax(dim=1)\n",
    "  val, ind = softmax(ll1(int_li[0])).sort(dim=1, descending=True)\n",
    "  # print(ind.shape)\n",
    "  top4 = ind[:,0:4,:]\n",
    "  top4 = top4.repeat(1, 1, 256)\n",
    "  # print(top4.shape)\n",
    "\n",
    "  # print(int_li[0][top4].shape)\n",
    "  # print(int_li[0].shape)\n",
    "  # print(int_li[0].gather(1, top4).shape)\n",
    "  res = int_li[0].gather(1, top4)\n",
    "  # print(res.shape)\n",
    "\n",
    "  val, ind = softmax(ll2(int_li[1])).sort(dim=1, descending=True)\n",
    "  top4 = ind[:,0:4,:]\n",
    "  top4 = top4.repeat(1,1,512)\n",
    "  res = torch.cat((res, int_li[1].gather(1, top4)), dim=2)\n",
    "  # print(res.shape)\n",
    "\n",
    "  val, ind = softmax(ll3(int_li[2])).sort(dim=1, descending=True)\n",
    "  top4 = ind[:,0:4,]\n",
    "  top4 = top4.repeat(1,1,1024)\n",
    "  res = torch.cat((res, int_li[2].gather(1, top4)), dim=2)\n",
    "\n",
    "  val, ind = softmax(ll4(int_li[3])).sort(dim=1, descending=True)\n",
    "  top4 = ind[:,0:4,]\n",
    "  top4 = top4.repeat(1,1,1024)\n",
    "  res = torch.cat((res, int_li[3].gather(1, top4)), dim=2)\n",
    "  print(res.shape)\n",
    "  return res\n",
    "\n",
    "get_kv(int_li).shape\n",
    "# ll2 = nn.Linear(512, 1, bias=False).to(device)\n",
    "# softmax = nn.Softmax(dim=1)\n",
    "# val, ind = softmax(ll2(int_li[1])).sort(dim=1, descending=True)\n",
    "# ind[0,0:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 4 \n",
    "inner_dim = 4//1\n",
    "group_queries = True\n",
    "group_key_values = True\n",
    "offset_groups = 1\n",
    "heads=4\n",
    "to_q = nn.Conv1d(144, inner_dim, 1, groups = offset_groups if group_queries else 1, bias = False).to(device)\n",
    "to_k = nn.Conv1d(dim, inner_dim, 1, groups = offset_groups if group_key_values else 1, bias = False).to(device)\n",
    "to_v = nn.Conv1d(dim, inner_dim, 1, groups = offset_groups if group_key_values else 1, bias = False).to(device)\n",
    "to_out = nn.Conv1d(inner_dim, 144, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "group = lambda t: rearrange(t, 'b (g d) n -> (b g) d n', g = offset_groups)\n",
    "grouped_queries = group(q)\n",
    "grouped_queries.shape #torch.Size([4, 49, 768])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2304, 1])\n",
      "tensor([0, 1, 2, 3], device='cuda:0')\n",
      "torch.Size([1, 4, 256])\n",
      "tensor([543, 542, 540, 541], device='cuda:0')\n",
      "torch.Size([1, 4, 512])\n",
      "tensor([63, 62, 60, 61], device='cuda:0')\n",
      "torch.Size([1, 4, 1024])\n",
      "tensor([63, 62, 60, 61], device='cuda:0')\n",
      "torch.Size([1, 4, 1024])\n",
      "torch.Size([1, 144, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.3214e+00, -9.9485e+00,  2.5861e+01,  ...,  2.1051e+01,\n",
       "          -1.7513e+00, -5.7222e-01],\n",
       "         [ 7.1564e+00,  2.3557e+01, -5.7981e+01,  ..., -6.0069e+01,\n",
       "           2.0151e+00,  1.1742e-01],\n",
       "         [ 1.0749e+00, -1.0574e+01,  3.6592e+01,  ...,  2.8776e+01,\n",
       "           2.5264e+00, -4.8475e-02],\n",
       "         ...,\n",
       "         [-5.2157e+00, -1.1454e+01,  2.2603e+01,  ...,  3.0666e+01,\n",
       "          -2.6834e+00, -4.5644e-01],\n",
       "         [ 6.8012e+00,  2.0441e+01, -5.4836e+01,  ..., -6.0974e+01,\n",
       "           2.3962e+00,  2.5008e-01],\n",
       "         [-1.5605e+00, -4.4011e+00, -1.5186e-01,  ..., -1.5263e+00,\n",
       "          -3.5369e-01,  3.0846e-01]]], device='cuda:0',\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def attention( out, int_li):\n",
    "  kv = get_kv(int_li)\n",
    "  k, v = to_k(kv), to_v(kv)\n",
    "  # print(k.shape)\n",
    "  # k.shape, v.shape #[1, 3168, 4])\n",
    "  q = to_q(out)\n",
    "  # q.shape #torch.Size([1, 196, 768])\n",
    "\n",
    "  # split out heads\n",
    "  q, k, v = map(lambda t: rearrange(t, 'b (h d) n -> b h n d', h = heads), (q, k, v))\n",
    "\n",
    "  # query / key similarity\n",
    "  sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "  # numerical stability\n",
    "\n",
    "  sim = sim - sim.amax(dim = -1, keepdim = True).detach()\n",
    "\n",
    "  # attention\n",
    "  dropout = nn.Dropout(0.0)\n",
    "  attn = sim.softmax(dim = -1)\n",
    "  attn = dropout(attn)\n",
    "  attn.shape\n",
    "\n",
    "  # aggregate and combine heads\n",
    "\n",
    "  out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "  out = rearrange(out, 'b h n d -> b (h d) n')\n",
    "  out = to_out(out)\n",
    "  print(out.shape)\n",
    "  return out\n",
    "attention(int_li[3], int_li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = COCO2017('../data/coco', phase='train')\n",
    "# partial=torch.utils.data.Subset(test_dataset, list(range(100)))\n",
    "# train_dataset = Voc2007Classification('data/voc', 'trainval', inp_name='data/voc/voc_glove_word2vec.pkl', LT=True)\n",
    "# test_dataset = Voc2007Classification('data/voc', 'test', inp_name='data/voc/voc_glove_word2vec.pkl')\n",
    "# train_dataset = COCO2014('data/coco', phase='train', inp_name='data/coco/coco_glove_word2vec.pkl')\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "train_dataset.transform = transforms.Compose([\n",
    "                Warp(384),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=50, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MultiLabelSoftMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = get_model(i).to(device)\n",
    "model = model.train()\n",
    "act_li = np.array([[]])\n",
    "for i, (input, target) in tqdm(enumerate(train_loader)):\n",
    "  img, path = input\n",
    "  target[target == 0] = 1\n",
    "  target[target == -1] = 0\n",
    "  feat_Var = torch.autograd.Variable(img).float().to(device)\n",
    "  \n",
    "  # output = model(feat_Var, None).detach()\n",
    "  # output = model(feat_Var)\n",
    "  output, int_li = forward_(model, feat_Var)\n",
    "  \n",
    "  out = attention(output, int_li)\n",
    "  out = out.mean(dim=1)\n",
    "  criterion(out, target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60887706f19020ebdc58f2aefdb30076f5f51d5973d281c7596168e0afd68511"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('LTML': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
