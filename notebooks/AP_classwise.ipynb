{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "import torchvision\n",
    "from voc import *\n",
    "from coco import *\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet152, resnet101, resnet18, resnet34, resnet50\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from config import seed_everything\n",
    "seed_everything(0)\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "from PIL import Image\n",
    "import json\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82081 {'file_name': 'COCO_train2014_000000057870.jpg', 'labels': [12, 77, 51, 22, 27]}\n",
      "34\n",
      "Counter({1: 1234, 2: 407, 3: 116, 4: 27, 5: 8, 8: 3, 7: 2, 6: 1})\n",
      "45174.0 128.0\n",
      "dict_keys([49, 22, 18, 27, 26, 13, 14, 36, 74, 2, 8, 12, 21, 57, 75, 24, 23, 41, 51, 28, 62, 72, 20, 16, 76, 68, 7, 32, 77, 58, 61, 42, 73, 44, 67, 65, 70, 9, 0, 10, 59, 50, 53, 11, 17, 37, 47, 5, 4, 34, 78, 52, 54, 64, 40, 3, 30, 33, 66, 39, 25, 15, 79, 45, 46, 63, 31, 19, 1, 60, 56, 43, 29, 38, 71, 55, 6, 48, 69, 35]) dict_values([45174, 8950, 8606, 8378, 6518, 5968, 5028, 4861, 4321, 3924, 3844, 3734, 3322, 3291, 3191, 3170, 3159, 3097, 3084, 3041, 2986, 2893, 2818, 2791, 2749, 2667, 2539, 2537, 2530, 2511, 2493, 2475, 2464, 2442, 2368, 2343, 2317, 2287, 2243, 2241, 2209, 2202, 2180, 2098, 2080, 2068, 2003, 1884, 1804, 1798, 1771, 1671, 1645, 1631, 1625, 1618, 1518, 1511, 1510, 1471, 1389, 1340, 1324, 1290, 1216, 1214, 1205, 1186, 1171, 1170, 1105, 1089, 1062, 821, 700, 673, 668, 481, 151, 128])\n"
     ]
    }
   ],
   "source": [
    "path_csv = '../data/coco'\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "class_num = defaultdict(int)\n",
    "with open(path_csv + '/data/train_anno.json') as f:\n",
    "  adj = np.zeros((80,80))\n",
    "  import json\n",
    "  train = json.load(f)\n",
    "  print(len(train), train[0])\n",
    "\n",
    "  li = []\n",
    "  gt_labels = np.zeros((len(train),80))\n",
    "  img_id2idx = dict()\n",
    "  idx2img_id = []\n",
    "  for i,each in enumerate(train):\n",
    "    li += each['labels']\n",
    "    gt_labels[i, each['labels']] = 1\n",
    "    for l in each['labels']:\n",
    "      class_num[l] += 1\n",
    "\n",
    "  nums = gt_labels.sum(axis=0)\n",
    "  adj = []\n",
    "  for i,col in enumerate(gt_labels.T):\n",
    "    if i in [34]:\n",
    "      print(i)\n",
    "      selected = gt_labels[np.isin(col, [1.0]), :]\n",
    "      nonzero_cnt = (selected != 0).sum(1)\n",
    "      cnter = Counter(nonzero_cnt)\n",
    "      print(cnter)\n",
    "    cond_prob = gt_labels[np.isin(col,[1.0]),:].sum(axis=0)\n",
    "    cond_prob[i] = 0\n",
    "    adj.append(cond_prob)\n",
    "    # print(adj[-1])\n",
    "  nums = nums.tolist()\n",
    "  nums.sort()\n",
    "  nums.reverse()\n",
    "  # nums = reversed(nums)\n",
    "  print(max(nums), min(nums))\n",
    "  di={'adj': np.asarray(adj), \"nums\": np.asarray(nums)}\n",
    "  class_di = {k: v for k, v in sorted(class_num.items(), key=lambda item: item[1], reverse=True)} #sorted\n",
    "print(class_di.keys(), class_di.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dataset] Done!\n",
      "[annotation] Done!\n",
      "[json] Done!\n"
     ]
    }
   ],
   "source": [
    "test_dataset = COCO2014('../data/coco', phase='val', inp_name='../data/coco/coco_glove_word2vec.pkl')\n",
    "# train_dataset = Voc2007Classification('data/voc', 'trainval', inp_name='data/voc/voc_glove_word2vec.pkl', LT=True)\n",
    "# test_dataset = Voc2007Classification('data/voc', 'test', inp_name='data/voc/voc_glove_word2vec.pkl')\n",
    "# train_dataset = COCO2014('data/coco', phase='train', inp_name='data/coco/coco_glove_word2vec.pkl')\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "test_dataset.transform = transforms.Compose([\n",
    "                MultiScaleCrop(224, scales=(1.0, 0.875, 0.75, 0.66, 0.5), max_distort=2),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "from util import AveragePrecisionMeter\n",
    "AP = AveragePrecisionMeter(difficult_examples=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'resnet50': 'resnetv2_50x3_bitm_in21k', 'vit': 'vit_base_patch16_224_in21k', 'swin': 'swin_base_patch4_window7_224_in22k', 'convnext': 'convnext_base_in22k', 'mlpmixer': 'mixer_b16_224_in21k'}\n",
      "resnet50 : resnetv2_50x3_bitm_in21k\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pathlib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/seongha/LT-ML/notebooks/AP_classwise.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/AP_classwise.ipynb#ch0000017vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/AP_classwise.ipynb#ch0000017vscode-remote?line=3'>4</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(k, v))\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/AP_classwise.ipynb#ch0000017vscode-remote?line=4'>5</a>\u001b[0m   pathlib\u001b[39m.\u001b[39mPath(\u001b[39m'\u001b[39m\u001b[39m../figures/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k))\u001b[39m.\u001b[39mmkdir(parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pathlib' is not defined"
     ]
    }
   ],
   "source": [
    "from backbones.config import config\n",
    "print(config)\n",
    "for k, v in config.items():\n",
    "  print(\"{} : {}\".format(k, v))\n",
    "  pathlib.Path('../figures/{}'.format(k)).mkdir(parents=True, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'resnet50': 'resnetv2_50x3_bitm_in21k', 'vit': 'vit_base_patch16_224_in21k', 'swin': 'swin_base_patch4_window7_224_in22k', 'convnext': 'convnext_base_in22k', 'mlpmixer': 'mixer_b16_224_in21k'}\n",
      "resnet50 : resnetv2_50x3_bitm_in21k\n",
      "vit : vit_base_patch16_224_in21k\n",
      "swin : swin_base_patch4_window7_224_in22k\n",
      "convnext : convnext_base_in22k\n",
      "mlpmixer : mixer_b16_224_in21k\n",
      "Linear(in_features=768, out_features=80, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "from backbones.config import config\n",
    "import pathlib\n",
    "\n",
    "print(config)\n",
    "for k, v in config.items():\n",
    "  print(\"{} : {}\".format(k, v))\n",
    "  pathlib.Path('../figures/{}'.format(k)).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "m_li = [base_resnet50(model_path=config['resnet50'], num_classes=80, pretrained=True),\\\n",
    "   base_vit(config['vit'], 80, image_size=224, pretrained=True),\\\n",
    "      base_swin(config['swin'], 80, image_size=224, pretrained=True),\\\n",
    "         base_convnext(config['convnext'], 80, image_size=224, pretrained=True), \\\n",
    "           base_mlpmixer(config['mlpmixer'], num_classes=80, image_size=224, pretrained=True)]\n",
    "# m_li2 = [BaseResnet(m_li[0], 80)]\n",
    "p_li = ['/home/seongha/LT-ML/checkpoint/coco/coco_LT(0)_label_cnt_in21k-4-4-0_resnet50_base_best.pth.tar', \\\n",
    "  '/home/seongha/LT-ML/checkpoint/coco/coco_LT(0)_label_cnt_in21k-4-4-0_vit_base_best.pth.tar', \\\n",
    "    '/home/seongha/LT-ML/checkpoint/coco/coco_LT(0)_label_cnt_in22k-4-4-0_swin_base_best.pth.tar',\\\n",
    "  '/home/seongha/LT-ML/checkpoint/coco/coco_LT(0)_label_cnt_in22k-4-4-0_convnext_base_best.pth.tar',\\\n",
    "      '/home/seongha/LT-ML/checkpoint/coco/coco_LT(0)_label_cnt_in21k-4-4-0_mlpmixer_base_best.pth.tar' ,\\\n",
    "  ]\n",
    "\n",
    "def get_model(index):\n",
    "  path = p_li[index]\n",
    "  model = m_li[index]\n",
    "  di = torch.load(path)\n",
    "  print(di['best_score'])\n",
    "  print(di.keys())\n",
    "  model.load_state_dict(di['state_dict'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mAP': tensor(75.3806), 'OF1': 0.7281215292811023, 'CF1': 0.706165627126658}\n",
      "dict_keys(['epoch', 'arch', 'state_dict', 'best_score'])\n",
      "{'mAP': tensor(29.3734), 'OF1': 0.35194173201084833, 'CF1': 0.15985432026494287}\n",
      "dict_keys(['epoch', 'arch', 'state_dict', 'best_score'])\n",
      "{'mAP': tensor(74.8108), 'OF1': 0.7193339297047961, 'CF1': 0.6759560773634149}\n",
      "dict_keys(['epoch', 'arch', 'state_dict', 'best_score'])\n",
      "{'mAP': tensor(75.5266), 'OF1': 0.7372472199137282, 'CF1': 0.7008232877486551}\n",
      "dict_keys(['epoch', 'arch', 'state_dict', 'best_score'])\n",
      "{'mAP': tensor(64.6940), 'OF1': 0.6456315916551508, 'CF1': 0.586278894601817}\n",
      "dict_keys(['epoch', 'arch', 'state_dict', 'best_score'])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "  model = get_model(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet50 : resnetv2_50x3_bitm_in21k\n",
      "vit : vit_base_patch16_224_in21k\n",
      "swin : swin_base_patch4_window7_224_in22k\n",
      "convnext : convnext_base_in22k\n",
      "mlpmixer : mixer_b16_224_in21k\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "for k, v in config.items():\n",
    "  print(\"{} : {}\".format(k, v))\n",
    "  pathlib.Path('../figures/{}'.format(k)).mkdir(parents=True, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlpmixer\n"
     ]
    }
   ],
   "source": [
    "i=4\n",
    "key_list = list(config)\n",
    "print(key_list[i])\n",
    "m_name = key_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mAP': tensor(74.8108), 'OF1': 0.7193339297047961, 'CF1': 0.6759560773634149}\n",
      "dict_keys(['epoch', 'arch', 'state_dict', 'best_score'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "402it [13:51,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40137, 80])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = get_model(i).to(device)\n",
    "model = model.eval()\n",
    "for i, (input, target) in tqdm(enumerate(test_loader)):\n",
    "  img, path, inp = input\n",
    "  target[target == 0] = 1\n",
    "  target[target == -1] = 0\n",
    "  feat_Var = torch.autograd.Variable(img).float().to(device)\n",
    "  \n",
    "  # output = model(feat_Var, None).detach()\n",
    "  output = model(feat_Var, None).detach()\n",
    "  # print(output.requires_grad, target.requires_grad)\n",
    "  # print(output.shape, target.shape)\n",
    "  AP.add(output, target)\n",
    "\n",
    "# map = 100 * AP.value().mean()\n",
    "# print(100 * AP.value())\n",
    "# ap_li = 100 * AP.value()\n",
    "print(AP.scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([840])\n",
      "torch.Size([491])\n",
      "torch.Size([1832])\n",
      "torch.Size([728])\n",
      "torch.Size([799])\n",
      "torch.Size([845])\n",
      "torch.Size([341])\n",
      "torch.Size([1292])\n",
      "torch.Size([1961])\n",
      "torch.Size([1114])\n",
      "torch.Size([1121])\n",
      "torch.Size([1048])\n",
      "torch.Size([1828])\n",
      "torch.Size([2912])\n",
      "torch.Size([2397])\n",
      "torch.Size([670])\n",
      "torch.Size([1350])\n",
      "torch.Size([969])\n",
      "torch.Size([4180])\n",
      "torch.Size([578])\n",
      "torch.Size([1480])\n",
      "torch.Size([1695])\n",
      "torch.Size([4404])\n",
      "torch.Size([1704])\n",
      "torch.Size([1448])\n",
      "torch.Size([666])\n",
      "torch.Size([3061])\n",
      "torch.Size([3960])\n",
      "torch.Size([1521])\n",
      "torch.Size([523])\n",
      "torch.Size([714])\n",
      "torch.Size([592])\n",
      "torch.Size([1173])\n",
      "torch.Size([757])\n",
      "torch.Size([849])\n",
      "torch.Size([70])\n",
      "torch.Size([2272])\n",
      "torch.Size([1001])\n",
      "torch.Size([452])\n",
      "torch.Size([750])\n",
      "torch.Size([727])\n",
      "torch.Size([1410])\n",
      "torch.Size([1232])\n",
      "torch.Size([512])\n",
      "torch.Size([1219])\n",
      "torch.Size([674])\n",
      "torch.Size([568])\n",
      "torch.Size([989])\n",
      "torch.Size([261])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/seongha/LT-ML/notebooks/AP_classwise.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/AP_classwise.ipynb#ch0000008vscode-remote?line=4'>5</a>\u001b[0m score_true \u001b[39m=\u001b[39m score[torch\u001b[39m.\u001b[39mwhere(target \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/AP_classwise.ipynb#ch0000008vscode-remote?line=5'>6</a>\u001b[0m \u001b[39m# print(torch.where(target==1))\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/AP_classwise.ipynb#ch0000008vscode-remote?line=7'>8</a>\u001b[0m dataset_true \u001b[39m=\u001b[39m [test_dataset[each] \u001b[39mfor\u001b[39;00m each \u001b[39min\u001b[39;00m torch\u001b[39m.\u001b[39mwhere(target \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/AP_classwise.ipynb#ch0000008vscode-remote?line=8'>9</a>\u001b[0m \u001b[39msorted\u001b[39m, indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msort(score_true, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, descending\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/AP_classwise.ipynb#ch0000008vscode-remote?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(score_true\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;32m/home/seongha/LT-ML/notebooks/AP_classwise.ipynb Cell 9'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/AP_classwise.ipynb#ch0000008vscode-remote?line=4'>5</a>\u001b[0m score_true \u001b[39m=\u001b[39m score[torch\u001b[39m.\u001b[39mwhere(target \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/AP_classwise.ipynb#ch0000008vscode-remote?line=5'>6</a>\u001b[0m \u001b[39m# print(torch.where(target==1))\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/AP_classwise.ipynb#ch0000008vscode-remote?line=7'>8</a>\u001b[0m dataset_true \u001b[39m=\u001b[39m [test_dataset[each] \u001b[39mfor\u001b[39;00m each \u001b[39min\u001b[39;00m torch\u001b[39m.\u001b[39mwhere(target \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/AP_classwise.ipynb#ch0000008vscode-remote?line=8'>9</a>\u001b[0m \u001b[39msorted\u001b[39m, indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msort(score_true, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, descending\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/AP_classwise.ipynb#ch0000008vscode-remote?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(score_true\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/LT-ML/notebooks/../coco.py:129\u001b[0m, in \u001b[0;36mCOCO2014.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    <a href='file:///home/seongha/LT-ML/notebooks/../coco.py?line=126'>127</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m    <a href='file:///home/seongha/LT-ML/notebooks/../coco.py?line=127'>128</a>\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_list[index]\n\u001b[0;32m--> <a href='file:///home/seongha/LT-ML/notebooks/../coco.py?line=128'>129</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget(item)\n",
      "File \u001b[0;32m~/LT-ML/notebooks/../coco.py:136\u001b[0m, in \u001b[0;36mCOCO2014.get\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    <a href='file:///home/seongha/LT-ML/notebooks/../coco.py?line=133'>134</a>\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m2014\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mphase), filename))\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='file:///home/seongha/LT-ML/notebooks/../coco.py?line=134'>135</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/seongha/LT-ML/notebooks/../coco.py?line=135'>136</a>\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    <a href='file:///home/seongha/LT-ML/notebooks/../coco.py?line=136'>137</a>\u001b[0m target \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes, np\u001b[39m.\u001b[39mfloat32) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/seongha/LT-ML/notebooks/../coco.py?line=137'>138</a>\u001b[0m target[labels] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=92'>93</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=93'>94</a>\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=94'>95</a>\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=95'>96</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/LTML/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/transforms.py:270\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=261'>262</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=262'>263</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=263'>264</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=264'>265</a>\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=267'>268</a>\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=268'>269</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=269'>270</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/functional.py:352\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/functional.py?line=346'>347</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/functional.py?line=347'>348</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001b[39m\u001b[39m{\u001b[39;00mtensor\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/functional.py?line=348'>349</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/functional.py?line=350'>351</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m inplace:\n\u001b[0;32m--> <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/functional.py?line=351'>352</a>\u001b[0m     tensor \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39;49mclone()\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/functional.py?line=353'>354</a>\u001b[0m dtype \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/torchvision/transforms/functional.py?line=354'>355</a>\u001b[0m mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(mean, dtype\u001b[39m=\u001b[39mdtype, device\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classwise_img = dict({\"class_index\":[], \"1\":[], \"2\":[], \"3\":[], \"-1\":[], \"-2\":[], \"-3\":[]})\n",
    "for i in range(AP.scores.shape[1]):\n",
    "  score = AP.scores[:,i]\n",
    "  target = AP.targets[:, i]\n",
    "  score_true = score[torch.where(target == 1)]\n",
    "  # print(torch.where(target==1))\n",
    "  \n",
    "  dataset_true = [test_dataset[each] for each in torch.where(target == 1)[0].tolist()]\n",
    "  sorted, indices = torch.sort(score_true, dim=0, descending=True)\n",
    "  \n",
    "  print(score_true.shape)\n",
    "  classwise_img[\"class_index\"].append(i)\n",
    "  classwise_img[\"1\"].append(dataset_true[int(indices[0])][0][1])\n",
    "  classwise_img[\"2\"].append(dataset_true[int(indices[1])][0][1])\n",
    "  classwise_img[\"3\"].append(dataset_true[int(indices[2])][0][1])\n",
    "  classwise_img[\"-1\"].append(dataset_true[int(indices[-1])][0][1])\n",
    "  classwise_img[\"-2\"].append(dataset_true[int(indices[-2])][0][1])\n",
    "  classwise_img[\"-3\"].append(dataset_true[int(indices[-3])][0][1])\n",
    "  \n",
    "classwise_img\n",
    "img_df = pd.DataFrame(data=classwise_img)\n",
    "img_df.to_csv(\"../figures/{}/topandworst.csv\".format(m_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27334/2526294419.py:3: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = plt.figure(figsize=(12, 12))\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams.update({'font.size': 8})\n",
    "for row_idx, row in img_df.iterrows():\n",
    "  fig = plt.figure(figsize=(12, 12))\n",
    "  for idx, (j, item) in enumerate(row.iteritems()):\n",
    "    if j == \"class_index\":\n",
    "      continue\n",
    "    a = fig.add_subplot(2, 3, idx)\n",
    "    img_id = '/home/seongha/LT-ML/data/coco/data/val2014/{}'.format(item)\n",
    "    image = Image.open(img_id)  \n",
    "    imgplot = plt.imshow(image)\n",
    "    # a.axis(\"off\")\n",
    "    # a.set_title(names[i].split('(')[0], fontsize=30)\n",
    "  \n",
    "  plt.savefig('../figures/{}/topandworst_class_{}.png'.format(m_name, row_idx), bbox_inches='tight', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classwise AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_li = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',\n",
    "       'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
    "       'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']#voc\n",
    "label_li = list(class_di.keys()) #coco\n",
    "ap_li = 100 * AP.value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sorted class wise ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>class_index</th>\n",
       "      <th>class_size</th>\n",
       "      <th>AP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>45174</td>\n",
       "      <td>95.046577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>8950</td>\n",
       "      <td>58.769329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>8606</td>\n",
       "      <td>35.414932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>8378</td>\n",
       "      <td>77.562492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>6518</td>\n",
       "      <td>91.629448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>55</td>\n",
       "      <td>673</td>\n",
       "      <td>74.701126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>6</td>\n",
       "      <td>668</td>\n",
       "      <td>70.634720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>48</td>\n",
       "      <td>481</td>\n",
       "      <td>66.823669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>69</td>\n",
       "      <td>151</td>\n",
       "      <td>58.696007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>35</td>\n",
       "      <td>128</td>\n",
       "      <td>96.634750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  class_index  class_size         AP\n",
       "0       0           49       45174  95.046577\n",
       "1       1           22        8950  58.769329\n",
       "2       2           18        8606  35.414932\n",
       "3       3           27        8378  77.562492\n",
       "4       4           26        6518  91.629448\n",
       "..    ...          ...         ...        ...\n",
       "75     75           55         673  74.701126\n",
       "76     76            6         668  70.634720\n",
       "77     77           48         481  66.823669\n",
       "78     78           69         151  58.696007\n",
       "79     79           35         128  96.634750\n",
       "\n",
       "[80 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ = {\"class_index\": list(class_di.keys()),\n",
    "\"class_size\": list(class_di.values()),\n",
    "\"AP\": ap_li}\n",
    "\n",
    "df = pd.DataFrame(data=data_)\n",
    "df.to_csv(\"../figures/{}/sorted_ap_{}.csv\".format(m_name, m_name))\n",
    "df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label distribution csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_name = 'convnext'\n",
    "df = pd.read_csv(\"../figures/{}/sorted_ap_{}.csv\".format(m_name, m_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0  class_index  class_size         AP\n",
      "67          67           19        1186  97.547930\n",
      "49          49           34        1798  97.169050\n",
      "34          34           67        2368  96.780850\n",
      "30          30           61        2493  95.840400\n",
      "79          79           35         128  94.742190\n",
      "73          73           38         821  93.922264\n",
      "0            0           49       45174  93.445090\n",
      "6            6           14        5028  92.356060\n",
      "19\n",
      "Counter({3: 290, 4: 239, 2: 210, 5: 160, 6: 88, 7: 67, 1: 36, 8: 31, 9: 28, 11: 14, 10: 12, 12: 9, 13: 1, 14: 1})\n",
      "34\n",
      "Counter({1: 1234, 2: 407, 3: 116, 4: 27, 5: 8, 8: 3, 7: 2, 6: 1})\n",
      "67\n",
      "Counter({3: 1083, 2: 688, 4: 388, 5: 146, 6: 44, 7: 9, 1: 4, 9: 3, 8: 2, 11: 1})\n",
      "61\n",
      "Counter({6: 395, 7: 375, 5: 352, 4: 346, 8: 267, 3: 219, 9: 179, 10: 110, 2: 85, 11: 81, 12: 45, 13: 20, 14: 8, 15: 5, 1: 4, 16: 2})\n",
      "35\n",
      "Counter({3: 42, 4: 27, 5: 22, 2: 17, 6: 10, 7: 3, 8: 3, 1: 3, 10: 1})\n",
      "38\n",
      "Counter({2: 209, 3: 181, 4: 126, 1: 81, 5: 75, 6: 66, 7: 38, 8: 19, 9: 13, 10: 6, 11: 5, 12: 1, 13: 1})\n",
      "49\n",
      "Counter({2: 15997, 3: 12123, 4: 7145, 5: 4298, 6: 2350, 7: 1388, 8: 747, 9: 436, 1: 238, 10: 228, 11: 124, 12: 57, 13: 27, 14: 9, 15: 6, 16: 1})\n",
      "14\n",
      "Counter({4: 853, 5: 731, 6: 690, 3: 680, 7: 596, 8: 404, 2: 350, 9: 311, 10: 180, 11: 115, 12: 57, 13: 28, 1: 15, 14: 9, 15: 6, 16: 2, 18: 1})\n",
      "65\n",
      "Counter({2: 1782, 3: 362, 4: 103, 5: 42, 1: 29, 6: 15, 7: 5, 8: 3, 10: 2})\n",
      "60\n",
      "Counter({2: 711, 3: 345, 4: 83, 5: 17, 1: 9, 6: 3, 7: 2})\n",
      "18\n",
      "Counter({3: 2552, 4: 2111, 5: 1385, 2: 1334, 6: 626, 7: 278, 8: 129, 1: 97, 9: 51, 10: 25, 11: 12, 12: 3, 13: 2, 14: 1})\n",
      "52\n",
      "Counter({3: 250, 5: 215, 4: 207, 6: 183, 2: 176, 7: 164, 8: 144, 9: 107, 1: 82, 10: 59, 11: 38, 12: 22, 13: 11, 14: 6, 15: 4, 16: 2, 18: 1})\n",
      "70\n",
      "Counter({2: 859, 1: 728, 3: 406, 4: 196, 5: 90, 6: 25, 7: 9, 11: 2, 10: 1, 8: 1})\n",
      "4\n",
      "Counter({3: 621, 4: 578, 2: 280, 5: 222, 6: 72, 7: 21, 1: 5, 8: 4, 9: 1})\n",
      "3\n",
      "Counter({2: 402, 3: 337, 4: 257, 1: 204, 5: 140, 6: 94, 7: 63, 8: 46, 9: 26, 10: 22, 11: 11, 12: 9, 13: 3, 14: 2, 16: 1, 15: 1})\n",
      "15\n",
      "Counter({3: 294, 2: 294, 1: 216, 4: 195, 5: 143, 6: 74, 7: 50, 8: 24, 9: 18, 10: 17, 11: 7, 12: 6, 13: 2})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "top = df.nlargest(8, 'AP')\n",
    "print(top)\n",
    "data_ = []\n",
    "for i, j in zip(top.class_index, top.index):\n",
    "  print(i)\n",
    "  col = gt_labels[:, i]\n",
    "  selected = gt_labels[np.isin(col, [1.0]), :]\n",
    "  nonzero_cnt = (selected != 0).sum(1)\n",
    "  cnter = Counter(nonzero_cnt)\n",
    "  print(cnter)\n",
    "  cnter = dict(sorted(cnter.items(),key = lambda i: i[0]))\n",
    "  cnter['class_index'] = i\n",
    "  cnter['rank'] = j\n",
    "  data_.append(cnter)\n",
    "df_top5 = pd.DataFrame(data=data_)\n",
    "df_top5 = df_top5.fillna(0)\n",
    "df_top5 = df_top5.set_index(['class_index', 'rank'])\n",
    "df_top5.to_csv(\"../figures/{}/label_distribution_top_{}.csv\".format(m_name, m_name))\n",
    "\n",
    "worst = df.nsmallest(8, 'AP')\n",
    "data_ = []\n",
    "for i, j in zip(worst.class_index, worst.index):\n",
    "  print(i)\n",
    "  col = gt_labels[:, i]\n",
    "  selected = gt_labels[np.isin(col, [1.0]), :]\n",
    "  nonzero_cnt = (selected != 0).sum(1)\n",
    "  cnter = Counter(nonzero_cnt)\n",
    "  print(cnter)\n",
    "  cnter = dict(sorted(cnter.items(),key = lambda i: i[0]))\n",
    "  cnter['class_index'] = i\n",
    "  cnter['rank'] = j\n",
    "  data_.append(cnter)\n",
    "df_worst5 = pd.DataFrame(data=data_)\n",
    "df_worst5 = df_worst5.fillna(0)\n",
    "df_worst5 = df_worst5.set_index(['class_index', 'rank'])\n",
    "df_worst5.to_csv(\"../figures/{}/label_distribution_worst_{}.csv\".format(m_name, m_name))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60887706f19020ebdc58f2aefdb30076f5f51d5973d281c7596168e0afd68511"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('LTML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
