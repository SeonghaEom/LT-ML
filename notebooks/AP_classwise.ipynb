{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seongha/anaconda3/envs/LTML/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "import torchvision\n",
    "from voc import *\n",
    "from coco import *\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet152, resnet101, resnet18, resnet34, resnet50\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from config import seed_everything\n",
    "seed_everything(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82081 {'file_name': 'COCO_train2014_000000057870.jpg', 'labels': [12, 77, 51, 22, 27]}\n",
      "34\n",
      "Counter({1: 1234, 2: 407, 3: 116, 4: 27, 5: 8, 8: 3, 7: 2, 6: 1})\n",
      "45174.0 128.0\n",
      "dict_keys([49, 22, 18, 27, 26, 13, 14, 36, 74, 2, 8, 12, 21, 57, 75, 24, 23, 41, 51, 28, 62, 72, 20, 16, 76, 68, 7, 32, 77, 58, 61, 42, 73, 44, 67, 65, 70, 9, 0, 10, 59, 50, 53, 11, 17, 37, 47, 5, 4, 34, 78, 52, 54, 64, 40, 3, 30, 33, 66, 39, 25, 15, 79, 45, 46, 63, 31, 19, 1, 60, 56, 43, 29, 38, 71, 55, 6, 48, 69, 35]) dict_values([45174, 8950, 8606, 8378, 6518, 5968, 5028, 4861, 4321, 3924, 3844, 3734, 3322, 3291, 3191, 3170, 3159, 3097, 3084, 3041, 2986, 2893, 2818, 2791, 2749, 2667, 2539, 2537, 2530, 2511, 2493, 2475, 2464, 2442, 2368, 2343, 2317, 2287, 2243, 2241, 2209, 2202, 2180, 2098, 2080, 2068, 2003, 1884, 1804, 1798, 1771, 1671, 1645, 1631, 1625, 1618, 1518, 1511, 1510, 1471, 1389, 1340, 1324, 1290, 1216, 1214, 1205, 1186, 1171, 1170, 1105, 1089, 1062, 821, 700, 673, 668, 481, 151, 128])\n"
     ]
    }
   ],
   "source": [
    "path_csv = '../data/coco'\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "class_num = defaultdict(int)\n",
    "with open(path_csv + '/data/train_anno.json') as f:\n",
    "  adj = np.zeros((80,80))\n",
    "  import json\n",
    "  train = json.load(f)\n",
    "  print(len(train), train[0])\n",
    "\n",
    "  li = []\n",
    "  gt_labels = np.zeros((len(train),80))\n",
    "  img_id2idx = dict()\n",
    "  idx2img_id = []\n",
    "  for i,each in enumerate(train):\n",
    "    li += each['labels']\n",
    "    gt_labels[i, each['labels']] = 1\n",
    "    for l in each['labels']:\n",
    "      class_num[l] += 1\n",
    "\n",
    "  nums = gt_labels.sum(axis=0)\n",
    "  adj = []\n",
    "  for i,col in enumerate(gt_labels.T):\n",
    "    if i in [34]:\n",
    "      print(i)\n",
    "      selected = gt_labels[np.isin(col, [1.0]), :]\n",
    "      nonzero_cnt = (selected != 0).sum(1)\n",
    "      cnter = Counter(nonzero_cnt)\n",
    "      print(cnter)\n",
    "    cond_prob = gt_labels[np.isin(col,[1.0]),:].sum(axis=0)\n",
    "    cond_prob[i] = 0\n",
    "    adj.append(cond_prob)\n",
    "    # print(adj[-1])\n",
    "  nums = nums.tolist()\n",
    "  nums.sort()\n",
    "  nums.reverse()\n",
    "  # nums = reversed(nums)\n",
    "  print(max(nums), min(nums))\n",
    "  di={'adj': np.asarray(adj), \"nums\": np.asarray(nums)}\n",
    "  class_di = {k: v for k, v in sorted(class_num.items(), key=lambda item: item[1], reverse=True)} #sorted\n",
    "print(class_di.keys(), class_di.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dataset] Done!\n",
      "[annotation] Done!\n",
      "[json] Done!\n"
     ]
    }
   ],
   "source": [
    "test_dataset = COCO2014('../data/coco', phase='val', inp_name='../data/coco/coco_glove_word2vec.pkl')\n",
    "# train_dataset = Voc2007Classification('data/voc', 'trainval', inp_name='data/voc/voc_glove_word2vec.pkl', LT=True)\n",
    "# test_dataset = Voc2007Classification('data/voc', 'test', inp_name='data/voc/voc_glove_word2vec.pkl')\n",
    "# train_dataset = COCO2014('data/coco', phase='train', inp_name='data/coco/coco_glove_word2vec.pkl')\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "test_dataset.transform = transforms.Compose([\n",
    "                MultiScaleCrop(224, scales=(1.0, 0.875, 0.75, 0.66, 0.5), max_distort=2),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
    "from util import AveragePrecisionMeter\n",
    "AP = AveragePrecisionMeter(difficult_examples=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=80, bias=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from models import *\n",
    "\n",
    "\n",
    "m_li = [base_resnet50(80, pretrained=True), base_vit(80, image_size=224, pretrained=True), base_swin(80, image_size=224, pretrained=True)]\n",
    "p_li = ['/home/seongha/LT-ML/checkpoint/coco/coco_LT(0)_label_count_resnet50-4-4-0_resnet50_base_best.pth.tar' ,\\\n",
    "  '/home/seongha/LT-ML/checkpoint/coco/coco_LT(0)_label_count_vit-4-4-0_vit_base_best.pth.tar', \\\n",
    "  '/home/seongha/LT-ML/checkpoint/coco/coco_LT(0)_label_count_swin-4-4-0_swin_base_best.pth.tar', \\\n",
    "  ]\n",
    "\n",
    "def get_model(index):\n",
    "  path = p_li[index]\n",
    "  model = m_li[index]\n",
    "  di = torch.load(path)\n",
    "  print(di['best_score'])\n",
    "  print(di.keys())\n",
    "  model.load_state_dict(di['state_dict'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mAP': tensor(62.4261), 'OF1': 0.6323788284730504, 'CF1': 0.5572570761842324}\n",
      "dict_keys(['epoch', 'arch', 'state_dict', 'best_score'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [03:10,  4.32s/it]"
     ]
    }
   ],
   "source": [
    "model = get_model(0)\n",
    "for i, (input, target) in tqdm(enumerate(test_loader)):\n",
    "  img, path, inp = input\n",
    "  target[target == 0] = 1\n",
    "  target[target == -1] = 0\n",
    "  feat_Var = torch.autograd.Variable(img).float().detach()\n",
    "  \n",
    "  # output = model(feat_Var, None).detach()\n",
    "  output = model(feat_Var, None).detach()\n",
    "  # print(output.requires_grad, target.requires_grad)\n",
    "  AP.add(output, target)\n",
    "map = 100 * AP.value().mean()\n",
    "print(100 * AP.value())\n",
    "ap_li = 100 * AP.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_li = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',\n",
    "       'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
    "       'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']#voc\n",
    "label_li = list(class_di.keys()) #coco\n",
    "\n",
    "#classifier\n",
    "ap_li = [75.4188, 76.3768, 66.3551, 79.1177, 45.1441, 62.4649, 83.6811, 85.9619,\n",
    "        58.1541, 48.6313, 67.8912, 79.3836, 79.2103, 69.3254, 88.5846, 50.0462,\n",
    "        26.8350, 57.0530, 75.6681, 71.1823]#FC\n",
    "ap_li2 = [77.9105, 76.8534, 71.2095, 80.9140, 45.2674, 63.6200, 84.0961, 86.4019,\n",
    "        59.8740, 52.9656, 68.0257, 80.5327, 79.3948, 71.0906, 88.7570, 49.5162,\n",
    "        33.0532, 55.2915, 76.8210, 70.2609]#Transformer\n",
    "ap_li_gcn = [44.9041, 51.7194, 41.8896, 52.5085, 31.0836, 42.8794, 75.6171, 61.9646,\n",
    "        39.8540, 26.9209, 45.8847, 50.5602, 48.6032, 48.8849, 78.4147, 37.8425,\n",
    "        15.7030, 46.4849, 45.6828, 53.3943]#gcn\n",
    "ap_li_tc = [52.6595, 59.2412, 48.0880, 60.5390, 34.5672, 49.0508, 77.6743, 70.0045,\n",
    "        42.0592, 32.3638, 49.0484, 57.0762, 55.9363, 55.3863, 81.9262, 41.5481,\n",
    "        18.1342, 48.6349, 52.5640, 58.6664]#TC\n",
    "\n",
    "#backbone;\n",
    "ap_li_resnet50_fc_coco = [44.8806, 18.7181,  4.2505, 26.0276,  1.8891,  2.0793, 55.0725, 29.3584,\n",
    "        21.6527, 13.7635, 34.6959, 50.7117,  4.5387,  9.8258, 10.3550, 42.5709,\n",
    "        39.2878, 19.0187, 29.4253, 10.1510, 57.6950, 11.8762, 13.9065, 42.1499,\n",
    "         4.6982, 33.9326, 10.8716, 15.9627, 32.3844, 19.4244, 61.4208, 29.5272,\n",
    "         3.1157,  3.6808, 75.4917,  0.1942,  5.2710, 19.7860, 15.4992, 16.0211,\n",
    "        15.0432,  3.9782, 21.1419,  8.8907, 30.8293,  1.9951, 32.1277, 21.3268,\n",
    "        20.2985, 83.6436, 46.0174,  5.8537, 20.1276,  5.9066, 20.2521, 12.6364,\n",
    "        54.7259, 21.3757,  3.1344,  6.6266,  3.5166,  3.2945,  3.5212, 31.7686,\n",
    "        15.5602, 11.4469, 40.4510,  2.9287, 17.3079,  0.1819, 36.1461,  2.6347,\n",
    "        30.3691, 31.7924, 28.6856, 21.0613, 17.1495, 20.7845,  3.5991, 73.5261]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_class_size(ap_li):\n",
    "  Sorted_Ap = []\n",
    "  for k, v in class_di.items():\n",
    "        idx = label_li.index(k)\n",
    "        print(\"{}, {}, {}\".format(k, v, ap_li[idx]))\n",
    "        Sorted_Ap.append( ap_li[idx])\n",
    "\n",
    "  return Sorted_Ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_map = {0: \"resnet50\", 1: \"vit\", 2: \"swin\"}\n",
    "with open(\"AP_{}_fc.txt\".format(name_map[0]), \"w\") as f:\n",
    "  f.write (str(sort_by_class_size(ap_li)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class wise ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"class_index\": list(class_di.keys()),\n",
    "\"class_size\": list(class_di.values()),\n",
    "\"AP\": ap_li}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label distribution csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60887706f19020ebdc58f2aefdb30076f5f51d5973d281c7596168e0afd68511"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('LTML': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
