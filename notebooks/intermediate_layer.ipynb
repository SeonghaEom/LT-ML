{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "import torchvision\n",
    "from voc import *\n",
    "from coco import *\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet152, resnet101, resnet18, resnet34, resnet50\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from config import seed_everything\n",
    "seed_everything(0)\n",
    "name_map = {0: \"resnet50\", 1: \"vit\", 2: \"swin\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82081 {'file_name': 'COCO_train2014_000000057870.jpg', 'labels': [12, 77, 51, 22, 27]}\n",
      "34\n",
      "Counter({1: 1234, 2: 407, 3: 116, 4: 27, 5: 8, 8: 3, 7: 2, 6: 1})\n",
      "45174.0 128.0\n",
      "dict_keys([49, 22, 18, 27, 26, 13, 14, 36, 74, 2, 8, 12, 21, 57, 75, 24, 23, 41, 51, 28, 62, 72, 20, 16, 76, 68, 7, 32, 77, 58, 61, 42, 73, 44, 67, 65, 70, 9, 0, 10, 59, 50, 53, 11, 17, 37, 47, 5, 4, 34, 78, 52, 54, 64, 40, 3, 30, 33, 66, 39, 25, 15, 79, 45, 46, 63, 31, 19, 1, 60, 56, 43, 29, 38, 71, 55, 6, 48, 69, 35]) dict_values([45174, 8950, 8606, 8378, 6518, 5968, 5028, 4861, 4321, 3924, 3844, 3734, 3322, 3291, 3191, 3170, 3159, 3097, 3084, 3041, 2986, 2893, 2818, 2791, 2749, 2667, 2539, 2537, 2530, 2511, 2493, 2475, 2464, 2442, 2368, 2343, 2317, 2287, 2243, 2241, 2209, 2202, 2180, 2098, 2080, 2068, 2003, 1884, 1804, 1798, 1771, 1671, 1645, 1631, 1625, 1618, 1518, 1511, 1510, 1471, 1389, 1340, 1324, 1290, 1216, 1214, 1205, 1186, 1171, 1170, 1105, 1089, 1062, 821, 700, 673, 668, 481, 151, 128])\n"
     ]
    }
   ],
   "source": [
    "path_csv = '../data/coco'\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "class_num = defaultdict(int)\n",
    "with open(path_csv + '/data/train_anno.json') as f:\n",
    "  adj = np.zeros((80,80))\n",
    "  import json\n",
    "  train = json.load(f)\n",
    "  print(len(train), train[0])\n",
    "\n",
    "  li = []\n",
    "  gt_labels = np.zeros((len(train),80))\n",
    "  img_id2idx = dict()\n",
    "  idx2img_id = []\n",
    "  for i,each in enumerate(train):\n",
    "    li += each['labels']\n",
    "    gt_labels[i, each['labels']] = 1\n",
    "    for l in each['labels']:\n",
    "      class_num[l] += 1\n",
    "\n",
    "  nums = gt_labels.sum(axis=0)\n",
    "  adj = []\n",
    "  for i,col in enumerate(gt_labels.T):\n",
    "    if i in [34]:\n",
    "      print(i)\n",
    "      selected = gt_labels[np.isin(col, [1.0]), :]\n",
    "      nonzero_cnt = (selected != 0).sum(1)\n",
    "      cnter = Counter(nonzero_cnt)\n",
    "      print(cnter)\n",
    "    cond_prob = gt_labels[np.isin(col,[1.0]),:].sum(axis=0)\n",
    "    cond_prob[i] = 0\n",
    "    adj.append(cond_prob)\n",
    "    # print(adj[-1])\n",
    "  nums = nums.tolist()\n",
    "  nums.sort()\n",
    "  nums.reverse()\n",
    "  # nums = reversed(nums)\n",
    "  print(max(nums), min(nums))\n",
    "  di={'adj': np.asarray(adj), \"nums\": np.asarray(nums)}\n",
    "  class_di = {k: v for k, v in sorted(class_num.items(), key=lambda item: item[1], reverse=True)} #sorted\n",
    "print(class_di.keys(), class_di.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dataset] Done!\n",
      "[annotation] Done!\n",
      "[json] Done!\n"
     ]
    }
   ],
   "source": [
    "# test_dataset = COCO2014('../data/coco', phase='val', inp_name='../data/coco/coco_glove_word2vec.pkl')\n",
    "# train_dataset = Voc2007Classification('data/voc', 'trainval', inp_name='data/voc/voc_glove_word2vec.pkl', LT=True)\n",
    "# test_dataset = Voc2007Classification('data/voc', 'test', inp_name='data/voc/voc_glove_word2vec.pkl')\n",
    "train_dataset = COCO2014('../data/coco', phase='train')\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "train_dataset.transform = transforms.Compose([\n",
    "                MultiScaleCrop(224, scales=(1.0, 0.875, 0.75, 0.66, 0.5), max_distort=2),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=False)\n",
    "from util import AveragePrecisionMeter\n",
    "AP = AveragePrecisionMeter(difficult_examples=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'resnet50': 'resnetv2_50x3_bitm_in21k', 'vit': 'vit_base_patch16_224_in21k', 'swin': 'swin_base_patch4_window7_224_in22k', 'convnext': 'convnext_base_in22k', 'mlpmixer': 'mixer_b16_224_in21k'}\n",
      "resnet50 : resnetv2_50x3_bitm_in21k\n",
      "vit : vit_base_patch16_224_in21k\n",
      "swin : swin_base_patch4_window7_224_in22k\n",
      "convnext : convnext_base_in22k\n",
      "mlpmixer : mixer_b16_224_in21k\n",
      "Linear(in_features=768, out_features=80, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "from backbones.config import config\n",
    "import pathlib\n",
    "\n",
    "print(config)\n",
    "for k, v in config.items():\n",
    "  print(\"{} : {}\".format(k, v))\n",
    "  pathlib.Path('../figures/{}'.format(k)).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "m_li = [base_resnet50(model_path=config['resnet50'], num_classes=80, pretrained=True),\\\n",
    "   base_vit(config['vit'], 80, image_size=224, pretrained=True),\\\n",
    "      base_swin(config['swin'], 80, image_size=224, pretrained=True),\\\n",
    "         base_convnext(config['convnext'], 80, image_size=224, pretrained=True), \\\n",
    "           base_mlpmixer(config['mlpmixer'], num_classes=80, image_size=224, pretrained=True), \\\n",
    "             base_resnet101(model_path=config['resnet50'], num_classes=80, pretrained=True)\n",
    "             ]\n",
    "# m_li2 = [BaseResnet(m_li[0], 80)]\n",
    "p_li = ['/home/seongha/LT-ML/checkpoint/coco/coco_LT(0)_label_cnt_in21k-4-4-0_resnet50_base_best.pth.tar', \\\n",
    "  '/home/seongha/LT-ML/checkpoint/coco/coco_LT(0)_label_cnt_in21k-4-4-0_vit_base_best.pth.tar', \\\n",
    "    '/home/seongha/LT-ML/checkpoint/coco/coco_LT(0)_label_cnt_in22k-4-4-0_swin_base_best.pth.tar',\\\n",
    "  '/home/seongha/LT-ML/checkpoint/coco/coco_LT(0)_label_cnt_in22k-4-4-0_convnext_base_best.pth.tar',\\\n",
    "      '/home/seongha/LT-ML/checkpoint/coco/coco_LT(0)_label_cnt_in21k-4-4-0_mlpmixer_base_best.pth.tar' ,\\\n",
    "  ]\n",
    "\n",
    "def get_model(index):\n",
    "  path = p_li[index]\n",
    "  model = m_li[index]\n",
    "  di = torch.load(path)\n",
    "  print(di['best_score'])\n",
    "  print(di.keys())\n",
    "  model.load_state_dict(di['state_dict'])\n",
    "\n",
    "  for n, p in model.named_parameters():\n",
    "    if p.requires_grad==False:\n",
    "      p.requires_grad=True\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool with window of size=3, stride=2\n",
    "m = nn.AvgPool1d(3, stride=2)\n",
    "m(torch.tensor([[[1.,2,3,4,5,6,7]]])) #1,1,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 768, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pool of square window of size=3, stride=2\n",
    "m = nn.AvgPool2d(3, stride=2)\n",
    "# pool of non-square window\n",
    "m = nn.AvgPool2d((63, 63), stride=(1, 1))\n",
    "input = torch.randn(20, 768, 63, 63)\n",
    "output = m(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=768, nhead=1)\n",
    "memory = torch.rand(32, 196, 768)\n",
    "m = nn.AvgPool2d((63, 63), stride=(1, 1))\n",
    "memory = m(memory)\n",
    "memory = torch.squeeze(memory, -1)\n",
    "memory = torch.squeeze(memory, -1)\n",
    "tgt = torch.rand(32, 768)\n",
    "out = decoder_layer(tgt, memory) #32,768\n",
    "\n",
    "m = nn.AvgPool1d(196, stride=1)\n",
    "out = m(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate symmetric padding for a convolution\n",
    "def get_padding(kernel_size: int, stride: int = 1, dilation: int = 1, **_) -> int:\n",
    "    padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2\n",
    "    return padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 24, 24])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With square kernels and equal stride\n",
    "m = nn.Conv2d(16, 33, 3, stride=2)\n",
    "# # non-square kernels and unequal stride and with padding\n",
    "# m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
    "# # non-square kernels and unequal stride and with padding and dilation\n",
    "# m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
    "input = torch.randn(20, 16, 50, 50)\n",
    "output = m(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mAP': tensor(75.5266), 'OF1': 0.7372472199137282, 'CF1': 0.7008232877486551}\n",
      "dict_keys(['epoch', 'arch', 'state_dict', 'best_score'])\n",
      "torch.Size([1, 1024, 7, 7])\n",
      "torch.Size([1, 256, 28, 28])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/seongha/LT-ML/notebooks/intermediate_layer.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/intermediate_layer.ipynb#ch0000010vscode-remote?line=10'>11</a>\u001b[0m out \u001b[39m=\u001b[39m seq(inp)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/intermediate_layer.ipynb#ch0000010vscode-remote?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(out\u001b[39m.\u001b[39mshape) \u001b[39m#[1, 784, 256], [1, 196, 512]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/intermediate_layer.ipynb#ch0000010vscode-remote?line=12'>13</a>\u001b[0m b, n, h \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "#convnext\n",
    "model = get_model(3)\n",
    "inp = torch.rand(3, 224, 224)\n",
    "inp = torch.unsqueeze(inp, 0)\n",
    "out = model.features(inp)\n",
    "print(out.shape) #[1, 1024, 7, 7])\n",
    "\n",
    "inp = torch.rand(3, 224, 224)\n",
    "inp = torch.unsqueeze(inp, 0)\n",
    "seq = nn.Sequential(*[model.features[0], model.features[1][0], model.features[1][1]]) #3-3-27-3 blocks\n",
    "out = seq(inp)\n",
    "print(out.shape) #[1, 256, 28, 28]\n",
    "b, n, h, w = out.shape\n",
    "\n",
    "ap2 = nn.AvgPool2d((28-7+1, 28-7+1), stride=(1,1))\n",
    "ap2(out)\n",
    "ap1 = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mAP': tensor(74.8108), 'OF1': 0.7193339297047961, 'CF1': 0.6759560773634149}\n",
      "dict_keys(['epoch', 'arch', 'state_dict', 'best_score'])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 196, 512])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 80])\n"
     ]
    }
   ],
   "source": [
    "#swin\n",
    "model = get_model(2)\n",
    "inp = torch.rand(3, 224, 224)\n",
    "inp = torch.unsqueeze(inp, 0)\n",
    "out = model.features(inp)\n",
    "print(out.shape)\n",
    "\n",
    "inp = torch.rand(3, 224, 224)\n",
    "inp = torch.unsqueeze(inp, 0)\n",
    "seq = nn.Sequential(*[model.features[0], model.features[1], model.features[2][0], model.features[2][1]]) #2-2-17-2 blocks\n",
    "out = seq(inp)\n",
    "print(out.shape) #[1, 784, 256], [1, 196, 512]\n",
    "b, n, h = out.shape\n",
    "out = out.reshape((1, -1))\n",
    "\n",
    "m = nn.AvgPool1d(n*h - 1024 + 1, stride=1)\n",
    "out = m(out).squeeze(-1)\n",
    "print(out.shape)\n",
    "\n",
    "logit = model.fc(out)\n",
    "print(logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vit\n",
    "model = get_model(1)\n",
    "inp = torch.rand(3, 224, 224)\n",
    "inp = torch.unsqueeze(inp, 0)\n",
    "out = model.features(inp)\n",
    "print(out.shape)\n",
    "\n",
    "inp = torch.rand(3, 224, 224)\n",
    "inp = torch.unsqueeze(inp, 0)\n",
    "seq = nn.Sequential(*[model.features[0], model.features[1], model.features[2][0], model.features[2][1], model.features[2][2]])\n",
    "out = seq(inp)\n",
    "print(out.shape) #([1, 196, 768])\n",
    "b, n, h = out.shape\n",
    "\n",
    "out = torch.swapaxes(out, 1,2)\n",
    "m = nn.AvgPool1d(196, stride=1)\n",
    "out = m(out).squeeze(-1)\n",
    "print(out.shape)\n",
    "\n",
    "logit = model.fc(out)\n",
    "print(logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/seongha/LT-ML/notebooks/intermediate_layer.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/intermediate_layer.ipynb#ch0000012vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m#resnet101\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/intermediate_layer.ipynb#ch0000012vscode-remote?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m get_model(\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/intermediate_layer.ipynb#ch0000012vscode-remote?line=2'>3</a>\u001b[0m inp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m3\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/LT-ML/notebooks/intermediate_layer.ipynb#ch0000012vscode-remote?line=3'>4</a>\u001b[0m inp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(inp, \u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_model' is not defined"
     ]
    }
   ],
   "source": [
    "#resnet101\n",
    "model = get_model(0)\n",
    "inp = torch.rand(3, 224, 224)\n",
    "inp = torch.unsqueeze(inp, 0)\n",
    "out = model.features(inp)\n",
    "out = model.head.global_pool(out)\n",
    "print(out.shape) #torch.Size([1, 6144, 1, 1])\n",
    "\n",
    "inp = torch.rand(3, 224, 224)\n",
    "inp = torch.unsqueeze(inp, 0)\n",
    "seq = nn.Sequential(*[model.features[0], model.features[1][0], model.features[1][1],model.features[1][2]])\n",
    "out = seq(inp)\n",
    "print(out.shape) #[1, 3072, 14, 14])\n",
    "b, n, h, w = out.shape\n",
    "out = out.reshape((1, -1))\n",
    "print(out.shape)\n",
    "\n",
    "m = nn.AvgPool1d(n*h*w - 6144 + 1, stride=1)\n",
    "# m = nn.Conv2d(n, )\n",
    "out = m(out)\n",
    "out = out.reshape((-1, 6144, 1, 1))\n",
    "print(out.shape)\n",
    "\n",
    "logit = model.head.fc(out)\n",
    "print(logit.shape)\n",
    "logit = logit.flatten()\n",
    "print(logit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mlpmixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(4)\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "inp = torch.rand(3, 224, 224)\n",
    "inp = torch.unsqueeze(inp, 0)\n",
    "seq = nn.Sequential(*[model.features[0], model.features[1][0]])\n",
    "out = seq(inp)\n",
    "print(out.shape)\n",
    "out = torch.swapaxes(out, 1,2)\n",
    "m = nn.AvgPool1d(196, stride=1)\n",
    "out = m(out).squeeze(-1)\n",
    "# out = torch.mean(out.view(out.size(0), out.size(1), -1), dim=2)\n",
    "torch.mean(torch.stack([out, out]), 1)\n",
    "print(out.shape)\n",
    "\n",
    "logit = model.fc(out)\n",
    "print(logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = model.features[1]\n",
    "# print(stages[3].blocks[2].norm3)\n",
    "target_layers = [stages[1].blocks[2].norm3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = get_model(0).to(device)\n",
    "model = model.eval()\n",
    "for i, (input, target) in tqdm(enumerate(test_loader)):\n",
    "  img, path = input\n",
    "  target[target == 0] = 1\n",
    "  target[target == -1] = 0\n",
    "  feat_Var = torch.autograd.Variable(img).float().to(device)\n",
    "  \n",
    "  # output = model(feat_Var, None).detach()\n",
    "  output = model(feat_Var, None).detach()\n",
    "  # print(output.requires_grad, target.requires_grad)\n",
    "  # print(output.shape, target.shape)\n",
    "  AP.add(output, target)\n",
    "\n",
    "# map = 100 * AP.value().mean()\n",
    "# print(100 * AP.value())\n",
    "# ap_li = 100 * AP.value()\n",
    "print(AP.scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60887706f19020ebdc58f2aefdb30076f5f51d5973d281c7596168e0afd68511"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('LTML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
